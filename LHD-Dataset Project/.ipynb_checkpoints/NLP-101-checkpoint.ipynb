{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustained-serial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in c:\\users\\singh\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (7.1.2)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\singh\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2020.11.13)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
      "Using legacy 'setup.py install' for nltk, since package 'wheel' is not installed.\n",
      "Installing collected packages: joblib, tqdm, nltk\n",
      "    Running setup.py install for nltk: started\n",
      "    Running setup.py install for nltk: finished with status 'done'\n",
      "Successfully installed joblib-1.0.0 nltk-3.5 tqdm-4.56.0\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "written-yellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contrary-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regular-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "democratic-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "middle-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "taken-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(data[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-settlement",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alternative-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "olympic-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document = \"\"\"\n",
    "We are having fun at major league hacking local hack day build! \n",
    "Feel free to join the discord channel. Also share about the session on social media.\n",
    "\"\"\"\n",
    "\n",
    "sentence = \"Send all the documents related to chapter 1,2,3,4 to utkarsh@xyz.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hourly-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endangered-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bronze-electricity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nWe are having fun at major league hacking local hack day build!',\n",
       " 'Feel free to join the discord channel.',\n",
       " 'Also share about the session on social media.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cooked-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return a tokenized copy of *text*,\n",
       "using NLTK's recommended word tokenizer\n",
       "(currently an improved :class:`.TreebankWordTokenizer`\n",
       "along with :class:`.PunktSentenceTokenizer`\n",
       "for the specified language).\n",
       "\n",
       ":param text: text to split into words\n",
       ":type text: str\n",
       ":param language: the model name in the Punkt corpus\n",
       ":type language: str\n",
       ":param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
       ":type preserve_line: bool\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = word_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "herbal-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence, language='english', preserve_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hidden-forest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapter',\n",
       " '1,2,3,4',\n",
       " 'to',\n",
       " 'utkarsh',\n",
       " '@',\n",
       " 'xyz.com']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-companion",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expensive-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "magnetic-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "streaming-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "altered-upper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "universal-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"has\" in sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "romance-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapter', '1,2,3,4', 'to', 'utkarsh@xyz.com']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Send all the documents related to chapter 1,2,3,4 to utkarsh@xyz.com\".split()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "specialized-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    useful_words = [word for word in text if word not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "circular-geneva",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send', 'documents', 'related', 'chapter', '1,2,3,4', 'utkarsh@xyz.com']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(sentence, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cognitive-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Send all the documents related to chapter 1,2,3,4 to utkarsh@xyz.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "twelve-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "personalized-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapter', 'to', 'utkarsh@xyz.com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text = tokenizer.tokenize(sentence)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-dependence",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "executive-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "I like this session and I liked all the sessions. I am liking everything. \n",
    "We are having fun at major league hacking local hack day build! \n",
    "Feel free to join the discord channel. also share about the session on social media.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "annual-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "included-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "latter-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('liking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "brave-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian cricket team will win World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "    'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the people',\n",
    "    'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "executive-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: sklearn in c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sklearn) (0.24.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\singh\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "induced-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "honey-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "integral-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "greek-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "stone-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus[0][40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "current-healthcare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'win': 38, 'world': 40, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 39, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "experienced-anaheim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
      " 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "substantial-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-debate",
   "metadata": {},
   "source": [
    "# Vectorisation with stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "liberal-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower()) # this is nltk one\n",
    "    # remove stopwords\n",
    "    words = remove_stopwords(words, sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "administrative-motion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer('this is some function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "female-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "favorite-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizedCorpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ignored-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "invisible-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizedCorpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "hourly-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 9, 'cricket': 3, 'team': 26, 'win': 30, 'world': 31, 'cup': 4, 'says': 22, 'capt.': 1, 'virat': 29, 'kohli.': 10, 'held': 8, 'sri': 24, 'lanka.': 11, 'next': 15, 'lok': 13, 'sabha': 21, 'elections': 5, 'confident': 2, 'pm': 18, 'nobel': 16, 'laurate': 12, 'hearts': 7, 'people': 17, 'movie': 14, 'raazi': 19, 'exciting': 6, 'spy': 23, 'thriller': 27, 'based': 0, 'upon': 28, 'real': 20, 'story': 25}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
